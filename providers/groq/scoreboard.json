{
  "warnings": [
    "Thinking models like qwen/qwen3-32b fails with tool calling when streaming. Currently disabled even not streaming in the client code.",
    "No models has consistent tool calling."
  ],
  "country": "US",
  "dashboardURL": "https://console.groq.com/dashboard/usage",
  "scenarios": [
    {
      "comments": "Is both indecisive and biased given its size and quantization.",
      "models": [
        "llama-3.1-8b-instant"
      ],
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      },
      "GenStream": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      }
    },
    {
      "comments": "70b is a bit better than 8b but not perfect.",
      "models": [
        "llama-3.3-70b-versatile"
      ],
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      },
      "GenStream": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      }
    },
    {
      "comments": "JSON only works when using ReasoningFormat: ReasoningFormatParsed",
      "models": [
        "deepseek-r1-distill-llama-70b"
      ],
      "thinking": true,
      "thinkingTokenStart": "\u003cthink\u003e",
      "thinkingTokenEnd": "\n\u003c/think\u003e\n",
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "true",
        "indecisiveTool": "flaky"
      },
      "GenStream": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "true",
        "indecisiveTool": "flaky"
      }
    },
    {
      "comments": "Sometimes tool calling fails",
      "models": [
        "meta-llama/llama-4-scout-17b-16e-instruct"
      ],
      "in": {
        "image": {
          "inline": true,
          "url": true,
          "supportedFormats": [
            "image/gif",
            "image/jpeg",
            "image/png",
            "image/webp"
          ]
        },
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      },
      "GenStream": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      }
    },
    {
      "comments": "Sometimes tool calling fails",
      "models": [
        "meta-llama/llama-4-maverick-17b-128e-instruct"
      ],
      "in": {
        "image": {
          "inline": true,
          "url": true,
          "supportedFormats": [
            "image/gif",
            "image/jpeg",
            "image/png",
            "image/webp"
          ]
        },
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      },
      "GenStream": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      }
    },
    {
      "comments": "JSON only works when using ReasoningFormat: ReasoningFormatParsed",
      "models": [
        "qwen/qwen3-32b"
      ],
      "thinking": true,
      "thinkingTokenStart": "\u003cthink\u003e",
      "thinkingTokenEnd": "\n\u003c/think\u003e\n",
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      },
      "GenStream": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky",
        "indecisiveTool": "flaky"
      }
    },
    {
      "comments": "Tool calling is flaky, it's mostly biased but tool calling doesn't always succeed.",
      "models": [
        "moonshotai/kimi-k2-instruct"
      ],
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky"
      },
      "GenStream": {
        "tools": "flaky",
        "json": true,
        "seed": true,
        "maxTokens": true,
        "stopSequence": true,
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "biasedTool": "flaky"
      }
    },
    {
      "comments": "Deprecated models.",
      "models": [
        "gemma2-9b-it",
        "llama3-70b-8192",
        "llama3-8b-8192"
      ]
    },
    {
      "comments": "Unsupported models.",
      "models": [
        "allam-2-7b",
        "compound-beta-mini",
        "compound-beta",
        "distil-whisper-large-v3-en",
        "meta-llama/llama-guard-4-12b",
        "meta-llama/llama-prompt-guard-2-22m",
        "meta-llama/llama-prompt-guard-2-86m",
        "openai/gpt-oss-20b",
        "openai/gpt-oss-120b",
        "playai-tts-arabic",
        "playai-tts",
        "whisper-large-v3-turbo",
        "whisper-large-v3"
      ]
    }
  ]
}

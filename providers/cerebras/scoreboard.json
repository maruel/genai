{
  "warnings": [
    "Cerebras doesn't support images yet even if models could. https://discord.com/channels/1085960591052644463/1376887536072527982",
    "Most models are quantized to unspecified level: https://discord.com/channels/1085960591052644463/1085960592050896937/1372105565655928864",
    "Free tier has limited context: https://inference-docs.cerebras.ai/support/pricing"
  ],
  "country": "US",
  "dashboardURL": "https://cloud.cerebras.ai",
  "scenarios": [
    {
      "models": [
        "zai-glm-4.6"
      ],
      "sota": true,
      "reason": true,
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "flaky",
        "toolsIndecisive": "true",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      },
      "GenStream": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "flaky",
        "toolsIndecisive": "true",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      }
    },
    {
      "models": [
        "qwen-3-235b-a22b-instruct-2507"
      ],
      "good": true,
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "true",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      },
      "GenStream": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "true",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      }
    },
    {
      "models": [
        "gpt-oss-120b"
      ],
      "cheap": true,
      "reason": true,
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "true",
        "topLogprobs": true,
        "maxTokens": true
      },
      "GenStream": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "true",
        "topLogprobs": true,
        "maxTokens": true
      }
    },
    {
      "comments": "qwen-3-32b has broken FinishReason when streaming, and doesn't support tool calling with streaming. The model is not quantized: https://discord.com/channels/1085960591052644463/1085960592050896937/1374399258890997830",
      "models": [
        "qwen-3-32b"
      ],
      "reason": true,
      "reasoningTokenStart": "\u003cthink\u003e",
      "reasoningTokenEnd": "\n\u003c/think\u003e\n",
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "true",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      },
      "GenStream": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "true",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      }
    },
    {
      "comments": "llama-3.1-8b works too but the ListModels() API returns the malformed string.",
      "models": [
        "llama3.1-8b"
      ],
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "true",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      },
      "GenStream": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsBiased": "true",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      }
    },
    {
      "models": [
        "llama-3.3-70b"
      ],
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "flaky",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      },
      "GenStream": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "tools": "true",
        "toolsIndecisive": "true",
        "toolCallRequired": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      }
    }
  ]
}

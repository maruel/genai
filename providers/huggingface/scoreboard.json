{
  "warnings": [
    "Huggingface supports a ton of models on its serverless inference platform, more than any other provider.",
    "Huggingface supports more options and modalities than what the client currently implement.",
    "Huggingface is also a router to other backends.",
    "Huggingface as a platform is generally unstable, with error not being properly reported. Use with care.",
    "Tool calling works very well but is biased; the model is lazy and when it's unsure, it will use the tool's first argument."
  ],
  "country": "US",
  "dashboardURL": "https://huggingface.co/settings/billing",
  "scenarios": [
    {
      "comments": "Untested",
      "models": [
        "deepseek-ai/DeepSeek-V3.1"
      ],
      "sota": true
    },
    {
      "comments": "Untested",
      "models": [
        "Qwen/Qwen3-Coder-480B-A35B-Instruct"
      ],
      "good": true
    },
    {
      "comments": "Untested",
      "models": [
        "meta-llama/Llama-3.2-1B-Instruct"
      ],
      "cheap": true
    },
    {
      "models": [
        "meta-llama/Llama-3.3-70B-Instruct"
      ],
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "json": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      },
      "GenStream": {
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "json": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      }
    },
    {
      "comments": "Seems to use a quantized version, tool calling is more flaky than would normally be expected. It doesn't follow JSON schema instructions.",
      "models": [
        "Qwen/Qwen3-4B"
      ],
      "reason": true,
      "reasoningTokenStart": "\u003cthink\u003e",
      "reasoningTokenEnd": "\u003c/think\u003e",
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "json": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      },
      "GenStream": {
        "reportRateLimits": true,
        "reportTokenUsage": "true",
        "reportFinishReason": "true",
        "seed": true,
        "json": true,
        "jsonSchema": true,
        "topLogprobs": true,
        "maxTokens": true,
        "stopSequence": true
      }
    }
  ]
}

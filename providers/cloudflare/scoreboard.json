{
  "warnings": [
    "FinishReason is not returned, ever.",
    "StopSequence doesn't work.",
    "Usage tokens isn't reported when streaming or using JSON.",
    "The structure of ChatRequest format is model dependent.",
    "Tool calling is supported on some models but it's flaky.",
    "Given the fact that FinishReason, StopSequence and Usage are broken, I can't recommend this provider beside toys."
  ],
  "country": "US",
  "dashboardURL": "https://dash.cloudflare.com",
  "scenarios": [
    {
      "comments": "Untested",
      "models": [
        "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b"
      ],
      "sota": true
    },
    {
      "comments": "Untested",
      "models": [
        "@cf/meta/llama-3.3-70b-instruct-fp8-fast"
      ],
      "good": true
    },
    {
      "comments": "Untested",
      "models": [
        "@cf/meta/llama-3.2-1b-instruct"
      ],
      "cheap": true
    },
    {
      "comments": "Llama-4 scout supports genai.ModalityImage but I'm not sure if cloudflare supports this modality. This may change in the future.",
      "models": [
        "@cf/meta/llama-4-scout-17b-16e-instruct"
      ],
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportTokenUsage": "true",
        "reportFinishReason": "flaky",
        "seed": true,
        "tools": "flaky",
        "json": true,
        "jsonSchema": true,
        "maxTokens": true
      },
      "GenStream": {
        "reportTokenUsage": "true",
        "reportFinishReason": "flaky",
        "seed": true,
        "json": true,
        "jsonSchema": true,
        "maxTokens": true
      }
    },
    {
      "models": [
        "@cf/meta/llama-3.2-3b-instruct"
      ],
      "in": {
        "text": {
          "inline": true
        }
      },
      "out": {
        "text": {
          "inline": true
        }
      },
      "GenSync": {
        "reportTokenUsage": "true",
        "reportFinishReason": "flaky",
        "seed": true,
        "tools": "flaky",
        "json": true,
        "maxTokens": true
      },
      "GenStream": {
        "reportTokenUsage": "true",
        "reportFinishReason": "flaky",
        "seed": true,
        "json": true,
        "maxTokens": true
      }
    }
  ]
}
